# Optimization

This folder contains **optimization algorithms implemented from scratch**
that are used to train machine learning models.

Optimization is the process of **minimizing a loss function** by iteratively
updating model parameters using gradients.

---

## Implemented Methods

### Gradient Descent
- Minimizes loss using first-order derivatives
- Updates parameters iteratively
- Forms the core of most ML training algorithms

ðŸ“„ File: `gradient_descent.py`

---

## Why Optimization Matters

All machine learning models learn by:
1. Making predictions
2. Measuring error using a loss function
3. Updating parameters to reduce that error

Optimization algorithms automate this learning process.

---

## Usage

The optimization code in this folder is designed to be:
- reusable across models
- easy to understand
- free from high-level ML libraries

It is used later in:
- Linear Regression
- Logistic Regression
- Neural Networks

